{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# Chains in LangChain\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. LLMChain\n",
    "- The simplest chain in LangChain.\n",
    "- It’s just:\n",
    "  - A PromptTemplate (instructions for the model)\n",
    "  - An LLM (like ChatOpenAI)\n",
    "  - An optional OutputParser (to structure the result)\n",
    "- LLMChain is like a single machine that takes an input and produces an output (e.g., a vending machine → put in a coin, get a snack)\n",
    "\n",
    "2. Sequential Chains\n",
    "- let you connect multiple chains together so the output of one becomes the input of the next.\n",
    "\n",
    "  2.1. SimpleSequentialChain\n",
    "  - Like a domino line — one output knocks into the next input.\n",
    "    - Connects chains in a straight line.\n",
    "    - Only passes a single string output from one chain to the next\n",
    "    - Very easy to use but limited.\n",
    "    \n",
    "  2.2. SequentialChain\n",
    "  - Like an assembly line in a factory — different stations process different parts, passing results along in an organized way\n",
    "    - More powerful: handles multiple inputs and outputs.\n",
    "    - Lets you specify which variable goes to which chain.\n",
    "    - Useful for more complex workflows\n",
    "\n",
    "3. Router Chain\n",
    "- Used when you have multiple specialized chains and you want the system to pick the right one based on the input.\n",
    "- It uses an LLM as a decision-maker to route the question\n",
    "- Like a receptionist in an office — listens to your request and sends you to the right department (translation desk, summary desk, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541eb2f1",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b61a3-92eb-4891-90ee-1d10607b05ad",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4336d784-65c2-4a11-8489-b445b1fad177",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b84e441b",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65dd20",
   "metadata": {},
   "source": [
    "- df stands for a Pandas DataFrame (a table-like data structure with rows and columns)\n",
    "- .head() - DataFrame method that returns the first 5 rows by default\n",
    "    - Useful when you want a quick look at your dataset without printing the whole table ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a09c35",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                                             Review\n",
       "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
       "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
       "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
       "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
       "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940ce7c",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9248c12a",
   "metadata": {},
   "source": [
    "The simplest chain in LangChain.\n",
    "- It’s just:\n",
    "  - A PromptTemplate (instructions for the model)\n",
    "  - An LLM (like ChatOpenAI)\n",
    "  - An optional OutputParser (to structure the result)\n",
    "- LLMChain is like a single machine that takes an input and produces an output (e.g., a vending machine → put in a coin, get a snack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92dff22",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943237a7",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe450c",
   "metadata": {},
   "source": [
    "- define templates with placeholders\n",
    "- prompt is now a reusable object ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdcdb42d",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9f347",
   "metadata": {},
   "source": [
    "- take your LLM and your prompt template and glue them together into a working LangChain chain\n",
    "- Separates prompt design from model execution\n",
    "- Lets you plug this chain into bigger workflows (like sequential chains or agents) ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7abc20b",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ea1dd",
   "metadata": {},
   "source": [
    "- chain.run(product) - call the LLMChain created earlier\n",
    "- .run() automatically\n",
    "    - inserts the input Quenn SIze Sheet Set into the prompt template\n",
    "    - sends formatted prompt to the llm model\n",
    "    - returns the model's generated answer as plain text ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad44d1fb",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Elevate Dream Beds\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Luxury Air Mattress\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03469",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febee243",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e047cb",
   "metadata": {},
   "source": [
    "1. define the model\n",
    "2. create prompt template\n",
    "3. build the chain \n",
    "\n",
    "This is a pipeline machine: input a product, get back a creative company name ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f31aa8a",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e07f3b",
   "metadata": {},
   "source": [
    "4. define another prompt template\n",
    "5. create another chain\n",
    "\n",
    "This is the second pipeline machine: input a company name, get back description ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5d5b76",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016f5ee",
   "metadata": {},
   "source": [
    "This is the step where the 2 chains connect into one pipeline, so they run automatically in sequence:\n",
    "- SimpleSequentialChain: A LangChain utility that links multiple chains together in a straight line.\n",
    "    - The output of one chain becomes the input of the next chain.\n",
    "    - Works only if each chain passes a single string forward (not multiple variables).\n",
    "- chains=[chain_one, chain_two]: \n",
    "    - Run chain_one first → generates a company name from the product. \n",
    "    - Then pass that output into chain_two → generates a 20-word description of the company.\n",
    "- verbose=True. Turns on debug mode.\n",
    "    - You’ll see in the console/notebook:\n",
    "        - The input each chain received.\n",
    "        - The output each chain produced.\n",
    "    - Super useful for understanding the flow ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c1eb2c4",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4f70c",
   "metadata": {},
   "source": [
    "- overall_simple_chain: This is your SimpleSequentialChain object\n",
    "    - It knows:\n",
    "        - Step 1 → use chain_one to create a company name.\n",
    "        - Step 2 → use chain_two to write a description from that name\n",
    "- .run(product)\n",
    "    - .run() is a convenience method for passing a single string input into the chain.\n",
    "    - Here product is your variable\n",
    "- The final output returned by .run(product) is the company description, not the intermediate name. ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78458efe",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mElite Slumber Co.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mElite Slumber Co. offers luxury mattresses and bedding for a restful night's sleep. Experience comfort and quality with us.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Elite Slumber Co. offers luxury mattresses and bedding for a restful night's sleep. Experience comfort and quality with us.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a80693",
   "metadata": {},
   "source": [
    "↑↑↑\n",
    "Why This Is Useful:\n",
    "- Lets you build multi-step pipelines easily.\n",
    "- You don’t have to manually take the output of chain_one and feed it into chain_two.\n",
    "- Great for workflows like:\n",
    "    - Summarize text → Translate summary → Extract keywords\n",
    "    - Generate company name → Write tagline → Suggest ad copy\n",
    "    - Product → Name → Description → Marketing Copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ce18c",
   "metadata": {},
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c129ef6",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "016187ac",
   "metadata": {
    "height": 217,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fb0730e",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6accf92d",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7a46121",
   "metadata": {
    "height": 217,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44caa96",
   "metadata": {},
   "source": [
    "- SequentialChain: LangChain class that links multiple chains in sequence\n",
    "    - you can pass/return multiple variables\n",
    "    - each chain can depend on different combinations of inputs/outputs\n",
    "- chains = [chain_one, chain_two, chain_three, chain_four]\n",
    "    - This means the overall pipeline will run four sub-chains in order.\n",
    "        - chain_one: Translate a review into English → produces \"English_Review\".\n",
    "        - chain_two: Summarize the review → produces \"summary\".\n",
    "        - chain_three: Analyze sentiment → might produce \"sentiment\".\n",
    "        - chain_four: Suggest a follow-up message → produces \"followup_message\"\n",
    "- input_variables=[\"Review\"]: tell the chain that the initial input is\n",
    "- output_variables=[\"English_Review\", \"summary\", \"followup_message\"]\n",
    "    - These are the final outputs you want after all chains run.\n",
    "    - So when you call overall_chain, you’ll get a dictionary ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89603117",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b212e",
   "metadata": {},
   "source": [
    "Run your multi-step SequentialChain on a real review from your dataset\n",
    "- df: is your Pandas DataFrame with a column called \"Review\".\n",
    "- df.Review[5]: selects the 6th row (index 5, since Python is 0-based)\n",
    "- When you pass in review, LangChain will:\n",
    "    - Take the raw review text as the \"Review\" input.\n",
    "    - Run it through each chain in sequence (chain_one → chain_two → chain_three → chain_four).\n",
    "    - Collect the final outputs in a dictionary ↓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51b04f45",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
       " 'summary': 'The reviewer finds the taste of the product mediocre, with poor foam retention, leading them to suspect that it might be an old batch or counterfeit.',\n",
       " 'followup_message': \"Je suis désolé d'entendre que vous avez trouvé le goût du produit médiocre et que la mousse ne se retient pas bien. Il est possible que vous ayez reçu un lot ancien ou contrefait. Je vous suggère de contacter le fabricant pour clarifier la situation et peut-être demander un remplacement si nécessaire. Merci de partager votre expérience avec nous.\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539fd7e9",
   "metadata": {},
   "source": [
    "↑\n",
    "↑\n",
    "↑\n",
    "Why This Matters\n",
    "- This shows how to apply your chain to real data instead of toy strings.\n",
    "- Perfect for analyzing many customer reviews in a dataset.\n",
    "- You can loop this over your whole df.Review column to generate structured outputs for every review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041ea4c",
   "metadata": {},
   "source": [
    "## Router Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a035073",
   "metadata": {},
   "source": [
    "- defining physics, maths, history and computer science personas ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ade83f4f",
   "metadata": {
    "height": 778,
    "tags": []
   },
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffbe833",
   "metadata": {},
   "source": [
    "- setting up a catalog of expert prompt options that can later be used in a RouterChain\n",
    "- prompt_infos: a list of dictionaries\n",
    "    - having them in a list makes it easy to loop over, feed into RouterChain, or dynamically build chains ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f590e9f",
   "metadata": {
    "height": 387,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31b06fc8",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f50bcc",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e1008",
   "metadata": {},
   "source": [
    "This code takes your expert prompt templates (prompt_infos) and actually turns them into runnable LLM chains, then prepares a list of destinations for a RouterChain.\n",
    "1. Create dictionary of chains destination_chains = {}\n",
    "2. loop over prompt_infos\n",
    "3. build a ChatPromptTemplate\n",
    "4. create an LLMCHain for each expert\n",
    "5. store in dictionary\n",
    "6. create a human-readable list of destinations\n",
    "7. join into one string ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eefec24",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191fd31",
   "metadata": {},
   "source": [
    "- setting up a default chain for your Router system. \n",
    "- It’s basically a fallback expert to handle cases where the router doesn’t know where to send the question ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f98018a",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a339fd4",
   "metadata": {},
   "source": [
    "- this is a router prompt template. \n",
    "- It’s what the LLM itself uses to decide which expert chain to route a question to ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11b2e2ba",
   "metadata": {
    "height": 506,
    "tags": []
   },
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of “destination” MUST match one of \\\n",
    "the candidate prompts listed below.\\\n",
    "If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e2473",
   "metadata": {},
   "source": [
    "- how the router LLM is actually built from the template you defined earlier\n",
    "1. Fill the router template\n",
    "2. Create a PromptTemplate\n",
    "3. Build the LLMRouterChain\n",
    "    Creates an LLMRouterChain that uses your chosen LLM (llm) to:\n",
    "        - Take the user input.\n",
    "        - Fill the router prompt (router_prompt).\n",
    "        - Ask the LLM to decide which expert chain to use.\n",
    "        - Parse the LLM’s JSON response with RouterOutputParser ↓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1387109d",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a92bc",
   "metadata": {},
   "source": [
    "- final assembly step where you connect the router with all your expert chains (physics, math, history, CS) and the fallback default chain\n",
    "- 1. MultiPromptChain:  LangChain class that combines:\n",
    "    - A router (decides which expert to use).\n",
    "    - A set of destination chains (the expert personas).\n",
    "    - A default chain (fallback if no expert fits)\n",
    "- router_chain=router_chain: The router you built earlier with LLMRouterChain\n",
    "- destination_chains=destination_chains: The dictionary of expert chains you created\n",
    "- default_chain=default_chain: A general-purpose fallback chain.\n",
    "    - If the router decides \"destination\": \"DEFAULT\", then this chain runs instead ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fb7d560",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d86b2131",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation is the electromagnetic radiation emitted by a perfect absorber and emitter of radiation, known as a black body. A black body absorbs all radiation that falls on it and emits radiation across the entire electromagnetic spectrum. The spectrum of black body radiation is continuous and depends only on the temperature of the black body. This phenomenon is described by Planck's law, which states that the intensity and wavelength distribution of the radiation emitted by a black body is determined solely by its temperature.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b717379",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29e5be01",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Every cell in our body contains DNA because DNA carries the genetic information that determines the characteristics and functions of an organism. DNA contains the instructions for building and maintaining an organism, including the proteins that are essential for cell function and structure. This genetic information is passed down from parent to offspring and is essential for the growth, development, and functioning of all cells in the body. Having DNA in every cell ensures that each cell has the necessary information to carry out its specific functions and contribute to the overall functioning of the organism.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4ec6a",
   "metadata": {},
   "source": [
    "my questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0653e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "computer science: {'input': 'What is Markov Chain?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A Markov Chain is a mathematical system that undergoes transitions from one state to another, with the probability of transitioning to any particular state depending solely on the current state. In other words, it is a stochastic process that satisfies the Markov property, which states that the future state of the system depends only on the present state and not on the sequence of events that preceded it.\\n\\nMarkov Chains are used in various fields such as statistics, economics, biology, and computer science to model random processes and make predictions about future states based on the current state. They are particularly useful in analyzing systems with a finite number of states and discrete time steps.\\n\\nIn computer science, Markov Chains are used in applications such as natural language processing, speech recognition, and image processing. They can be used to generate random sequences of events, simulate complex systems, and analyze the behavior of algorithms.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is Markov Chain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25bfe127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'Explain black holes?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Black holes are regions in space where gravity is so strong that nothing, not even light, can escape from them. They are formed when a massive star collapses in on itself, creating a singularity at its center. The gravitational pull of a black hole is so intense that it warps space and time around it, creating a point of no return called the event horizon. Anything that crosses the event horizon is pulled into the black hole and crushed into the singularity at its center. Black holes come in different sizes, with supermassive black holes found at the centers of galaxies and stellar black holes formed from the remnants of massive stars.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Explain black holes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8841309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "computer science: {'input': 'What is the best statistical method for understanding which marketing channels bring ROI?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The best statistical method for understanding which marketing channels bring ROI is typically through the use of regression analysis. Regression analysis allows you to analyze the relationship between different marketing channels and the return on investment they bring. By collecting data on the various marketing channels used, the corresponding ROI, and other relevant factors, you can use regression analysis to determine which channels are most effective in driving ROI.\\n\\nAdditionally, you can also use techniques such as A/B testing, cohort analysis, and attribution modeling to further understand the impact of different marketing channels on ROI. These methods can help you identify the most effective channels for your specific business and make data-driven decisions to optimize your marketing strategy.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is the best statistical method for understanding which marketing channels bring ROI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67d40fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "History: {'input': 'When did Croatia enter EU?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Croatia officially became a member of the European Union on July 1, 2013.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"When did Croatia enter EU?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ae23226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'How many legs do spiders have?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spiders have eight legs.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"How many legs do spiders have?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
