{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073bf8f9",
   "metadata": {},
   "source": [
    "# LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    " * Direct API calls to OpenAI\n",
    " * API calls through LangChain:\n",
    "   * Prompts\n",
    "   * Models\n",
    "   * Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ff606",
   "metadata": {},
   "source": [
    "## Get your [OpenAI API Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af5896",
   "metadata": {},
   "source": [
    "dotenv is to be able to read env file - where the api key is stored ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70aa2619",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install python-dotenv\n",
    "#%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a92fb-8227-4513-8950-c965b822c425",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4336d784-65c2-4a11-8489-b445b1fad177",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9cdb",
   "metadata": {},
   "source": [
    "## Chat API : OpenAI\n",
    "\n",
    "Let's start with a direct API calls to OpenAI.\n",
    "This is a helper function for sending a prompt to an OpenAI chat model and getting back just the text of the response\n",
    "- def get_completion(prompt, model=llm_model) → Defines a function named get_completion. It takes two arguments:\n",
    "- prompt → the user’s input (string).\n",
    "- model → defaults to the variable llm_model (set earlier in your code to \"gpt-3.5-turbo\" or similar).\n",
    "- messages = … → OpenAI chat models expect a list of messages with roles (\"system\", \"user\", \"assistant\"). Here, we create a single-user message containing the prompt: [{\"role\": \"user\", \"content\": \"Write a haiku about AI\"}]\n",
    "- response = … → Calls the OpenAI API using the ChatCompletion endpoint. Parameters:\n",
    "model=model → which LLM to use (e.g. \"gpt-3.5-turbo\").\n",
    "- messages=messages → the conversation history (just one message here).\n",
    "- temperature=0 → makes the response more deterministic (less randomness).\n",
    "\n",
    "- return … → The API returns a response object with a list of possible completions (choices).\n",
    "- choices[0] → take the first completion.\n",
    "- .message[\"content\"] → grab just the text of the assistant’s reply.\n",
    "Finally, return that text as the function’s output.\n",
    "\n",
    "This function:\n",
    "- Wraps your text into the format the OpenAI API expects.\n",
    "- Sends it to the model you specify.\n",
    "- Returns just the assistant’s reply text (not the whole JSON response) ↓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484bfa6a",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d076ce",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91889303",
   "metadata": {},
   "source": [
    "Course customer_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b32b57a",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130a8ec",
   "metadata": {},
   "source": [
    "My customer_email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e852bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "this is super stupid \\\n",
    "do only stupid people work at \\\n",
    "your stupid company,\\\n",
    "your stupid lid \\\n",
    "fucked up my kitchen \\\n",
    "i want a frickin refund!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c34459",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b558e2",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "this is super stupid do only stupid people work at your stupid company,your stupid lid fucked up my kitchen i want a frickin refund!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c883dcbd",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b33f61",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am disappointed with the experience I had with your company. The lid I purchased ended up causing damage to my kitchen. I would like a refund for this issue. Thank you.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80482d1",
   "metadata": {},
   "source": [
    "## Chat API : LangChain\n",
    "\n",
    "Let's try how we can do the same using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a525b58",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c5b27",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86246b",
   "metadata": {},
   "source": [
    "Why use ChatOpenAI in LangChain?\n",
    "- It standardizes the interface for different LLM providers (so your code looks the same whether you use OpenAI, Anthropic, Cohere, etc.).\n",
    "- It makes it easy to plug models into LangChain chains, agents, and tools.\n",
    "- It hides some of the boilerplate (messages=[...]) you had to write with the raw OpenAI API.\n",
    "\n",
    "\n",
    " This code creates a LangChain LLM object (chat) that wraps an OpenAI chat model (like GPT-3.5 or GPT-4) with deterministic behavior (temperature=0). You can now pass prompts/messages to it in a more structured way ↓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d4a269",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc0c8b8",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d07256",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57bda7d8",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5f3e1",
   "metadata": {},
   "source": [
    "It lets you define prompts with variables (placeholders) that you can fill in later.\n",
    "- .from_template(...) converts that string into a ChatPromptTemplate object.\n",
    "\n",
    "The result is not plain text — it’s a template object you can use to format prompts safely and repeatedly ↓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a31f246",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b3445",
   "metadata": {},
   "source": [
    "- ↑ Instead of hardcoding full prompts, you can reuse and fill them with different inputs.\n",
    "- This code takes a string with placeholders and turns it into a prompt template object that LangChain can use to reliably generate prompts for LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0efcb",
   "metadata": {},
   "source": [
    "- messages is a list of message templates inside your chat prompt.\n",
    "- Each element corresponds to a role in the conversation (like “system”, “user”, or “assistant”).\n",
    "- In this case, since you built it from a simple string, you’ll likely just have one user message ↓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cac2cb16",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3acab",
   "metadata": {},
   "source": [
    "- prompt_template = a ChatPromptTemplate (container for message templates).\n",
    "- prompt_template.messages[0] = the first message template (likely a HumanMessagePromptTemplate).\n",
    "- .prompt = the underlying PromptTemplate object (the raw text with {sentence}).\n",
    "- .input_variables = Every PromptTemplate keeps track of which placeholders (variables inside {}) are required to format the string.\n",
    "- This code asks the first message template which placeholders it needs. It’s basically saying:\n",
    "- “Hey template, what variables do I need to fill in before you’re ready to send to the LLM?”\n",
    "- Lets you confirm what inputs you must provide when calling .format_messages().\n",
    "- Prevents errors — if you forget to supply a required variable, LangChain will raise an exception ↓\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdc5566c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba896d",
   "metadata": {},
   "source": [
    "1. Define style and the customer’s email ↓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbd51a93",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa5452",
   "metadata": {},
   "source": [
    "Course customer_email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48989d11",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475130fd",
   "metadata": {},
   "source": [
    "My customer_email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "800c816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "this is super stupid \\\n",
    "do only stupid people work at \\\n",
    "your stupid company,\\\n",
    "your stupid lid \\\n",
    "fucked up my kitchen \\\n",
    "i want a frickin refund!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5838467",
   "metadata": {},
   "source": [
    "2. Format the template with variables\n",
    "- earlier you had a ChatPromptTemplate with placeholders like {style} and {text}.\n",
    "- This call fills in those placeholders with customer_style and customer_email.\n",
    "- The result: a list of LangChain Message objects (like HumanMessage) ↓\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dff3954f",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c09d8b4",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e02dafa2",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nthis is super stupid do only stupid people work at your stupid company,your stupid lid fucked up my kitchen i want a frickin refund!\\n```\\n' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e6a93",
   "metadata": {},
   "source": [
    "3. Send the message to the LLM\n",
    "- Calls your ChatOpenAI instance (chat) with the filled messages.\n",
    "- The model rewrites the pirate rant in calm/respectful American English.\n",
    "- .content prints just the text reply ↓\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd789f9f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad294407",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am disappointed with the quality of your product. The lid that I purchased from your company caused damage to my kitchen. I would like a refund for this issue. Thank you.\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098b6c4",
   "metadata": {},
   "source": [
    "Course service_reply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c267e5f",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05563a17",
   "metadata": {},
   "source": [
    "My service_reply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1766f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there stupid, \\\n",
    "who you calling stupid \\\n",
    "do you think I like working here \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "you will not be getting any refunds any time soon. \\\n",
    "Tough luck sucker!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7fa60e",
   "metadata": {},
   "source": [
    "(course)A new desired service agent response style ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ff72bd1",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053356f6",
   "metadata": {},
   "source": [
    "(my) desired service agent response style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "897c46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in nice English\\\n",
    "warm and friendly\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ac8a2",
   "metadata": {},
   "source": [
    "Fills the template again ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d9e8f3f",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in nice Englishwarm and friendly. text: ```Hey there stupid, who you calling stupid do you think I like working here because it's your fault that you misused your blender by forgetting to put the lid on before you will not be getting any refunds any time soon. Tough luck sucker!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c0fa8",
   "metadata": {},
   "source": [
    "The LLM rewrites the rude service message into a polite pirate-style response ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0ae5552",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there, my dear. I must kindly ask you to refrain from using such harsh language. It seems there may have been a misunderstanding regarding the blender. Unfortunately, refunds will not be possible at this time. Thank you for your understanding.\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58784c4f",
   "metadata": {},
   "source": [
    "↑ This code shows how to:\n",
    "1. Define a prompt template with placeholders for {style} and {text}.\n",
    "2. Fill it with a customer complaint (pirate rant) and a desired tone (calm, respectful).\n",
    "3. Use ChatOpenAI to translate the text into that tone.\n",
    "4. Do the same with a service reply, translating it into polite pirate English.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36536e79",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7027d",
   "metadata": {},
   "source": [
    "- In Python, this is a dictionary (a data type that stores key–value pairs).\n",
    "- In JSON, it’s an object (very similar — JSON is often used for saving or sending data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1ccdff5",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df0f4680",
   "metadata": {
    "height": 540,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73095b7",
   "metadata": {},
   "source": [
    "1. This imports the ChatPromptTemplate class from LangChain.\n",
    "- ChatPromptTemplate is used to build prompts for chat-based models (like GPT).\n",
    "- Instead of writing long prompts directly as strings, you can make templates with placeholders.\n",
    "2. create an prompt from a template\n",
    "- from_template(...) takes a string template (e.g., with variables) and turns it into a LangChain ChatPromptTemplate object.\n",
    "- review_template is a string variable defined earlier\n",
    "- When you later format the template, you fill in {review_text} with actual input.\n",
    "3. print the structure of the template ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2386e9c",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "121bb0d4",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10de1d28",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a3c0609",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You will get an error by running this line of code \n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "# response.content.get('gift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7de2b8",
   "metadata": {},
   "source": [
    "### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f496d1d",
   "metadata": {},
   "source": [
    "1. ResponseSchema\n",
    "- A blueprint for one piece of output you expect from the model.\n",
    "- You define a schema by naming the field and describing what should go inside\n",
    "2. StructuredOutputParser\n",
    "- takes a ResponseSchema and builds a parser\n",
    "- parser gives the model instructions on how to format the output (usually JSON)\n",
    "- parser parses the model's raw text into a Python dictionary ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2e0ec49",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55aaab",
   "metadata": {},
   "source": [
    "- we want a field called gift (y/n)\n",
    "- we want a field \"delivery_days\"\n",
    "- field \"price_value\" ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dea24b4",
   "metadata": {
    "height": 353,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b57e1ba8",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599434af",
   "metadata": {},
   "source": [
    "- output_parser was created from your ResponseSchemas.\n",
    "- When you call .get_format_instructions(), the parser generates special text instructions that you can insert into your prompt for the LLM.\n",
    "- These instructions tell the LLM exactly how to format its answer — usually as JSON with the fields you defined ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdeaf4fc",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f71c6b",
   "metadata": {},
   "source": [
    "- ↑ Instead of you writing long instructions manually, get_format_instructions() auto-generates them from your schema.\n",
    "- You then add format_instructions into your PromptTemplate so the LLM knows exactly how to respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1eb176c5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be67849",
   "metadata": {},
   "source": [
    "- ↑ LLMs normally return free-form text → messy for code.\n",
    "- ResponseSchema + StructuredOutputParser force the model to answer in structured JSON, so your program can reliably use the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007f910",
   "metadata": {},
   "source": [
    "1. Define the Prompt Template (review_template_2 = \"\"\"...\"\"\")\n",
    "- Clear guidance for each field (gift, delivery_days, price_value).\n",
    "- A placeholder {text} → where the actual customer review will go.\n",
    "- A placeholder {format_instructions} → where the JSON formatting rules (from output_parser.get_format_instructions()) will be inserted\n",
    "2. Turn Template into a ChatPromptTemplate\n",
    "- ChatPromptTemplate is a LangChain helper for chat-style prompts.\n",
    "- This lets you later substitute variables ({text}, {format_instructions}) with real values\n",
    "3. Format the Prompt with Variables\n",
    "- Here you fill in the blanks:\n",
    "- {text} → replaced with the actual customer_review string.\n",
    "- {format_instructions} → replaced with the formatting rules from your StructuredOutputParser.\n",
    "- The result (messages) is a list of chat messages ready to send to an LLM ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "082947fc",
   "metadata": {
    "height": 370,
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f5228",
   "metadata": {},
   "source": [
    "- *messages* is a list of LangChain BaseMessage objects (usually HumanMessage, sometimes SystemMessage or AIMessage).\n",
    "- Each message has two important parts:\n",
    "    - role → who the message is from (human/system/AI).\n",
    "    - content → the actual text of the message.\n",
    "- *messages[0]* gets the first message in the list.\n",
    "- In your case, that’s the human instruction prompt you just built.\n",
    "- *.content* extracts just the text content of that message ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f1537a7",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf337ba",
   "metadata": {},
   "source": [
    "- this line is where you actually send your prompt to the LLM ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b663657",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8c3a9fe",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": 2,\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e81dd",
   "metadata": {},
   "source": [
    "**Turning the LLM’s raw text output into a usable Python dictionary ↓**\n",
    "- response.content - This is the raw string output from the LLM (likely JSON)\n",
    "- output_parser.parse(...) - Converts that raw string into a Python dictionary\n",
    "- output_dict - Stores the parsed dictionary so you can reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "904f1c25",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8c2e33",
   "metadata": {},
   "source": [
    "- In a Jupyter Notebook, the last line of a cell is automatically displayed as output.\n",
    "- So instead of *print(output_dict)*, you can just write *output_dict* ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d48b647a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4346150f",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d781a",
   "metadata": {},
   "source": [
    "- pull values out of the parsed dictionary\n",
    "- .get('delivery_days')\n",
    "    - .get() is a dictionary method in Python.\n",
    "    - It looks for the key 'delivery_days' in the dictionary.\n",
    "    - If found → returns the value.\n",
    "    - If not found → returns None (instead of raising an error, which would happen if you used output_dict['delivery_days'])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a833fcea",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain_course)",
   "language": "python",
   "name": "langchain_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
